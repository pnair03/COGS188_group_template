{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# COGS 188 - Project Proposal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Names\n",
    "\n",
    "Hopefully your team is at least this good. Obviously you should replace these with your names.\n",
    "\n",
    "- Sripad Karne\n",
    "- Pranav Nair\n",
    "- Vishal Patel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Abstract \n",
    "\n",
    "The project is going to be a mimic of the game Pac-Man. The goal is to have Pac-Man eat all the yellow pellets, big pellets, and fruit. This should be done while avoiding being eaten by the ghosts. If a maze is cleared, a new maze will be generated, and the score will keep running. Pac-Man will also have three lives, and he will lose a life each time he touches a ghost. The game is over when all three lives have been lost. When a big pellet is eaten, Pac-Man is allowed to eat the ghosts for an added number of points. No data is necessary for this project to be completed. The success will be measured by the number of points a player earns. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Background\n",
    "\n",
    "Pac-Man is a classic example of a maze-based game with dynamic enemy behaviors and strategic gameplay events, so it has naturally been a popular game to test AI research on. Additionally, research related to AI-based gameplay has seen significant improvements in recent years. \n",
    "Prior work has utilized various AI algorithms and techniques to create a bot that can optimally beat the game and obtain a high score. One barrier that made Pac-Man a difficult problem to solve was the constantly changing boards and moving parts. The topic was explored by researchers at the University of Loannina who proposed that an informative state space description was important to help design efficient RL agents needed to solve Pac-Man and all games similar.<a name=\"ms.pac-man\"></a>[<sup>[1]</sup>](#ms.pac-man)\n",
    "Researchers from Maluuba, a startup that was acquired by Microsoft, used reinforcement learning to achieve the maximum score possible of 999,990 on the Atari 2600 version of Ms. Pac-Man<a name=\"linn\"></a>[<sup>[2]</sup>](#linn). The team essentially divided this into small problems to solve and then split these into tasks that were eventually completed by AI agents. This process, known as Hybrid Reward Architecture, used over 150 different AI agents to fully beat Ms. Pac-Man<a name=\"linn\"></a>[<sup>[2]</sup>](#linn). The belief was that this form of reinforcement learning would allow these agents to “think” and make their own decisions, which would allow engineers to work on more complex and seemingly higher-value projects and tasks. This sort of reinforcement learning isn’t the only thing to be explored; Deep Reinforcement Learning from Juman Feedback (DRLHF), as well as Cooperative Inverse Reinforcement Learning (CIRL), have both been explored to allow for an AI-controlled Pac-Man to learn from extremely limited human feedback and find an optimal solution to the game<a name=\"box\"></a>[<sup>[3]</sup>](#box). DRLHF has many applications, and it allows the Pac-Man agent to first learn from trained human players, interact with the environment on its own, and be shaped by the human feedback it receives when exploring the game environment. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem Statement\n",
    "\n",
    "We are trying to solve the problem of making Pacman fully autonomous using AI algorithms and techniques. This would mean we want Pacman to fully decide, based on current environmental conditions, what the next steps should be and to maximize its score by getting as many points as possible before beating the game or dying 3 times. To do this, we would utilize AI algorithms such as the Markov Decision Process to find the most optimal next action, while also using some kind of search algorithms to search for the ghosts and coins in the maze. This task can be quantifiable by representing the board as our environment with $n$ number of configurations or states. The action space of the Pacman would be 4 cardinal directions with some constrictions depending on the surrounding environment. The problem can be measured through the number of total points obtained at the end of the game. The metric we can use is the higher the score (number of points) the more successful the AI performed on the task, and should be optimized to keep reaching this score or higher. This problem is replicable since in each game of Pacman you play the same problem occurs, where you are trying to get Pacman to beat the game or to get the most points possible before dying. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data\n",
    "\n",
    "In Pacman, the rewards could be determined by the rules and structure of the game. We could also use the same deduction and addition of points to determine the reward function. The following reward breakdown is as follows: Pac-dot is +10 points, Power Pellet is +50 points, 1st Ghost is +200 points, 2nd Ghost is +400 points, 3rd Ghost is +800 points, 4th Ghost is 1600 points, Cherry is +100 points, Strawberry is +300 points, Orange is +500 points, Apple is +700 points, Melon is +1000 points, Galaxian is +2000 points, Bell is +3000 points, and key is +5000 points. To help guide Pacman in not dying from touching Ghosts, the act of colliding with the ghost can have a large negative reward such as -100 points. To encourage Pacman to make optimal decisions, but also care more about surviving than taking risks to get a higher score, we can implement a small reward of around +1 for each step Pacman takes that keeps it alive. To encourage Pacman to finish the maze, we can have a large positive reward to finish the maze of +500 points. Based on this reward mechanism, the implemented Pacman would be encouraged to take optimal strategies to stay alive and go for higher rewarded points, while also staying alive and finishing the game."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Proposed Solution\n",
    "\n",
    "To achieve full autonomy for Pac-Man, we will employ AI algorithms, notably the Markov Decision Process (MDP), combined with search algorithms tailored to Pac-Man's environment. MDP will enable Pac-Man to make optimal decisions based on current conditions and be able to maximize its score by collecting pellets, avoiding ghosts, and strategically consuming power pellets. Search algorithms will assist in navigating the maze and tracking the locations of both ghosts and all consumable items (including the ghosts when they are consumable). The solution will be implemented using a combination of Python libraries, leveraging existing frameworks such as OpenAI Gym or Pygame for Pac-Man simulation, and custom implementations of MDP and search algorithms. Testing whether the Pac-Man simulation works won’t be too difficult; games can be run through terminal commands to see if everything operates as expected. Checking if the greatest possible score is attained, however, is more challenging, as this depends on all of the ghosts’ movements. These movements aren’t random, even within the same map, but many trials need to be run to verify if the maximum is being attained. This solution can be compared to the scores of successful players of Pac-Man, but this cannot be considered a benchmark model.   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation Metrics\n",
    "\n",
    "One evaluation metric we can use to measure the performance of the benchmark and solution model is the total cumulative reward. This cumulative reward would be the sum of all the rewards for each model’s iterations. For each episode in the model, we can keep track of the cumulative reward where a higher number would mean a better performance by the AI, and a lower score would mean a worse performance. We can use these total cumulative rewards to compare the performance and how successful the model is. For example, If the benchmark model has a total reward of roughly 100, while the solution model has a total reward of roughly 10, this could indicate that the benchmark model is better at collecting more points with fewer consequences, while the solution model tends to face more consequences of undesired actions. \n",
    "Another evaluation metric that can be used to measure the performance of the benchmark and solution model is the average score per game. This average score per game can be formulated by the formula: $\\frac{1}{N}\\sum_{i=1}^N E_i$, where N is the total number of games and $E_i$ would be the ith episode. The score of the game could be captured by the number of coins and points the user gets in one game. By capturing the average score per game, we can compare it with other models and determine that a higher average score means that the model performed relatively better, while lower scores indicate the model performed relatively worse.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ethics & Privacy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Seeing as no data is going to be used, there aren’t any major concerns with ethics or data privacy for this project. A potential concern would be the AI recording data related to how a player plays the game, but even this isn’t a major concern because the AI will likely be implemented using a separate algorithm. \n",
    "A potential concern is algorithmic bias. Seeing as the AI algorithms used in this game will be written by people, we will likely introduce our own biases into the algorithms, which may lead to the AI-controlled Pac-Man favoring a certain direction, part of the gameboard, etc. \n",
    "Another concern is the subject of intellectual property. From a general sense, there isn’t much separating this game from the original Pac-Man or any other Pac-Man games that are currently available for download. This game, however, will not be available for purchase and is for use in this project, and it has already been acknowledged that this game is a copy of the original game. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Team Expectations "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* We will communicate effectively through group chats, this ensures that each member is immediately notified and has the ability to respond quickly.\n",
    "* A lot of programming will be done asynchronously, which will mitigate the effects of scheduling conflicts. Should there be any, we will discuss all time possibilities for when we will meet and prioritize this project.\n",
    "* Making decisions will always be done as a team, and creating goals will be done on a weekly basis.\n",
    "* We expect every person to put in an equal amount of work into the project.\n",
    "* If we have a conflict within our team we would gather all members together and talk it out. Communication is the best way to solve any problem and we would all take action needed collectively."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project Timeline Proposal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| Meeting Date  | Meeting Time| Completed Before Meeting  | Discuss at Meeting |\n",
    "|---|---|---|---|\n",
    "| 5/10  |  4-6 PM |  Work on assigned project proposal tasks  | Discuss remaining project proposal tasks and  complete and submit the Project Proposal | \n",
    "| 5/13  |  6-7 PM |  Do Independent Research and get a technical understanding of how the project will work | Discuss the main project objectives as well as which algorithm and tools needed to achieve them and assign individual tasks to be completed before the next in person meeting | \n",
    "| 5/16  | 5-9 PM  | Having Completed the Tasks assigned in last meeting  | Go through the tasks you have completed and the work you have done since the last meeting on Monday and discuss next steps needed. Also continue to work on the project and get a good rough draft/outline of what needs to be done and set objectives. Assign new tasks or work needed to be completed .   |\n",
    "| 5/17  | 5-9 PM  | Finish Assigned Tasks  | Continue working on the Draft and discuss any potential issues/limitations.   |\n",
    "| 5/23  | 5-9 PM  | Working on the Rough Draft | Discuss and edit project code and be almost done with rough draft |\n",
    "| 5/24  | 5-9 PM  | Working on the Rough Draft| Continue working on the Draft and have a finalized draft done by the end of the meeting |\n",
    "| 5/27  | 7-8 PM  | Go through finalized Rough Draft and record any bugs/issues | Discuss final bugs or issues and make sure we have hit all the objectives that we set. Assign final tasks to finish the Project Up  |\n",
    "| 5/30  | 5-9 PM  | Finish Assigned Tasks | Go through all the fixes you’ve made and discuss and make sure everything is done, finalizing everything. Submit the Final Project!  |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Footnotes\n",
    "<a name=\"ms.pac-man\"></a>1.[^](#ms.pac-man): Play Ms. Pac-Man Using an Advanced Reinforcement Learning, [www.lix.polytechnique.fr/~ntziortziotis/pubs/SETN14.pdf](https://www.lix.polytechnique.fr/~ntziortziotis/pubs/SETN14.pdf)<br> \n",
    "<a name=\"linn\"></a>2.[^](#linn): Linn, A. (14 June 2017) Divide and conquer: How mnicrosoft researchers used AI to master Ms. Pac-Man. Microsoft Blogs https://blogs.microsoft.com/ai/divide-conquer-microsoft-researchers-used-ai-master-ms-pac-man/#:~:text=Microsoft%20researchers%20have%20created%20an,tasks%20that%20augment%20human%20capabilities.<br>\n",
    "<a name=\"box\"></a>3.[^](#box): Box, C. (8 Aug 2023) Pac-Man as an AI Agent: A Blast from the Past Aesthetic in the Reinforcement Learning from Human Feedback Process, Medium, https://medium.com/@celeste_box/pac-man-as-an-ai-agent-a-blast-from-the-past-aesthetic-in-the-reinforcement-learning-from-human-39a1fb6de624 "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
